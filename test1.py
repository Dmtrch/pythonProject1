# Импортируем библиотеку numpy
import numpy as np

# Задаем параметры нейронной сети
n_inputs = 2 # Количество входов
n_outputs = 1 # Количество выходов
n_hidden = 1 # Количество скрытых нейронов
learning_rate = 0.01 # Скорость обучения
epochs = 1000 # Количество эпох обучения

# Инициализируем веса и смещения случайными значениями
W1 = np.random.randn(n_hidden, n_inputs) # Веса между входным и скрытым слоем
b1 = np.random.randn(n_hidden) # Смещения скрытого слоя
W2 = np.random.randn(n_outputs, n_hidden) # Веса между скрытым и выходным слоем
b2 = np.random.randn(n_outputs) # Смещения выходного слоя

# Определяем функцию активации (сигмоида)
def sigmoid(x):
  return 1 / (1 + np.exp(-x))

# Определяем производную функции активации
def sigmoid_prime(x):
  return sigmoid(x) * (1 - sigmoid(x))

# Определяем функцию потери (среднеквадратичная ошибка)
def mse(y_true, y_pred):
  return np.mean((y_true - y_pred) ** 2)

# Определяем производную функции потери
def mse_prime(y_true, y_pred):
  return 2 * (y_pred - y_true) / y_true.size

# Создаем обучающий набор данных
# X - матрица входных данных, где первый столбец - температура по цельсию, а второй - единица
# Y - вектор выходных данных, где каждый элемент - температура по фаренгейту
X = np.array([[0, 1], [10, 1], [20, 1], [30, 1], [40, 1]])
Y = np.array([[32], [50], [68], [86], [104]])

# Начинаем обучение нейронной сети
for i in range(epochs):
  # Прямое распространение
  # Вычисляем выходы скрытого слоя
  Z1 = X.dot(W1.T) + b1 # Линейная комбинация входов и весов, плюс смещения
  A1 = sigmoid(Z1) # Применяем функцию активации
  # Вычисляем выходы выходного слоя
  Z2 = A1.dot(W2.T) + b2 # Линейная комбинация скрытых выходов и весов, плюс смещения
  A2 = sigmoid(Z2) # Применяем функцию активации
  # Вычисляем значение функции потери
  loss = mse(Y, A2) # Сравниваем реальные и предсказанные значения
  # Выводим значение функции потери каждые 100 эпох
  if i % 100 == 0:
    print(f"Epoch {i}, Loss: {loss}")

  # Обратное распространение
  # Вычисляем градиенты функции потери по выходам
  E2 = mse_prime(Y, A2) # Производная функции потери по выходам
  dZ2 = E2 * sigmoid_prime(Z2) # Производная выходов по Z2
  # Вычисляем градиенты функции потери по весам и смещениям выходного слоя
  dW2 = dZ2.T.dot(A1) # Производная Z2 по W2
  db2 = np.sum(dZ2, axis=0) # Производная Z2 по b2
  # Вычисляем градиенты функции потери по скрытым выходам
  E1 = dZ2.dot(W2) # Производная функции потери по скрытым выходам
  dZ1 = E1 * sigmoid_prime(Z1) # Производная скрытых выходов по Z1
  # Вычисляем градиенты функции потери по весам и смещениям скрытого слоя
  dW1 = dZ1.T.dot(X) # Производная Z1 по W1
  db1 = np.sum(dZ1, axis=0) # Производная Z1 по b1
  # Обновляем веса и смещения с учетом скорости обучения
  W2 = W2 - learning_rate * dW2
  b2 = b2 - learning_rate * db2
  W1 = W1 - learning_rate * dW1
  b1 = b1 - learning_rate * db1

# Тестируем нейронную сеть на новых данных
X_test = np.array([[15, 1], [25, 1], [35, 1]])
# Прямое распространение
Z1 = X_test.dot(W1.T) + b1
A1 = sigmoid(Z1)
Z2 = A1.dot(W2.T) + b2
A2 = sigmoid(Z2)
# Выводим результаты
print("Input (Celsius):")
print(X_test[:, 0])
print("Output (Fahrenheit):")
print(A2 * 100)

